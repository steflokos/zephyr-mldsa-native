#!/usr/bin/env python3
# Copyright (c) The mlkem-native project authors
# Copyright (c) The mldsa-native project authors
# SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT

import subprocess
import tempfile
import platform
import argparse
import shutil
import pathlib
import re
import sys
import pyparsing as pp
import os
import yaml

from concurrent.futures import ThreadPoolExecutor
from functools import partial

modulus = 8380417
root_of_unity = 1753
montgomery_factor = pow(2, 32, modulus)

# This file re-generated auto-generated source files in mldsa-native.
#
# It currently covers:
# - zeta values for the reference NTT and invNTT
# - header guards


# Standard color definitions
GREEN = "\033[32m"
RED = "\033[31m"
BLUE = "\033[94m"
BOLD = "\033[1m"
NORMAL = "\033[0m"


def clear_status_line():
    """Clear any existing status line by overwriting with spaces and returning to start of line"""
    print(f"\r{' ' * 160}", end="", flush=True)


def status_update(task, msg):
    clear_status_line()
    print(f"\r{BLUE}[{task}]{NORMAL}: {msg} ...", end="", flush=True)


def high_level_status(msg):
    clear_status_line()
    print(
        f"\r{GREEN}âœ“{NORMAL} {msg}"
    )  # This will end with a newline, clearing the status line


def info(msg):
    clear_status_line()
    print(f"\r{GREEN}info{NORMAL} {msg}")


def error(msg):
    clear_status_line()
    print(f"\r{RED}error{NORMAL} {msg}")


def file_updated(filename):
    clear_status_line()
    print(f"\r{BOLD}updated {filename}{NORMAL}")


def gen_header():
    yield "/*"
    yield " * Copyright (c) The mldsa-native project authors"
    yield " * SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT"
    yield " */"
    yield ""
    yield "/*"
    yield " * WARNING: This file is auto-generated from scripts/autogen"
    yield " *          in the mldsa-native repository."
    yield " *          Do not modify it directly."
    yield " */"
    yield ""


def gen_yaml_header():
    yield "# Copyright (c) The mldsa-native project authors"
    yield "# SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT"
    yield ""


def get_files(pattern):
    return [str(p) for p in pathlib.Path().glob(pattern) if p.is_file()]


def get_c_source_files(main_only=False):
    if main_only is True:
        return get_files("mldsa/**/*.c")
    else:
        return get_files("**/*.c")


def get_asm_source_files(main_only=False):
    if main_only is True:
        return get_files("mldsa/**/*.S")
    else:
        return get_files("**/*.S")


def get_header_files(main_only=False):
    if main_only is True:
        return get_files("mldsa/**/*.h")
    else:
        return get_files("**/*.h")


def format_content(content):
    p = subprocess.run(
        ["clang-format"], capture_output=True, input=content, text=True, shell=True
    )
    if p.returncode != 0:
        print(p.stderr)
        print(
            f"Failed to auto-format autogenerated code (clang-format return code {p.returncode}). Are you running in a nix shell? See BUILDING.md."
        )
        exit(1)
    return p.stdout


class CondParser:
    """Rudimentary parser for expressions if `#if .. #else ..` directives"""

    def __init__(self):
        c_identifier = pp.common.identifier()
        c_integer_suffix = pp.one_of("U L LU UL LL ULL LLU", caseless=True)
        c_dec_integer = pp.Combine(
            pp.Optional(pp.one_of("+ -"))
            + pp.Word(pp.nums)
            + pp.Optional(c_integer_suffix)
        )
        c_hex_integer = pp.Combine(
            pp.Literal("0x") + pp.Word(pp.hexnums) + pp.Optional(c_integer_suffix)
        )

        self.parser = pp.infix_notation(
            c_identifier | c_hex_integer | c_dec_integer,
            [
                (pp.one_of("!"), 1, pp.opAssoc.RIGHT),
                (pp.one_of("!= == <= >= > <"), 2, pp.opAssoc.LEFT),
                (pp.one_of("&&"), 2, pp.opAssoc.LEFT),
                (pp.one_of("||"), 2, pp.opAssoc.LEFT),
            ],
        )

    @staticmethod
    def connective(res):
        """Extract the top-level connective for the expression"""
        if not isinstance(res, list):
            return None
        elif len(res) == 2:
            # Unary operator (will be "!" in our case)
            return res[0]
        else:
            # Binary operator
            return res[1]

    @staticmethod
    def map_top(f, res):
        """Apply function to arguments of top-level connective"""
        if not isinstance(res, list):
            return res
        else:
            # We expect `f` to do nothing on strings, so it is safe
            # to apply it everywhere, including the connectives.
            return list(map(f, res))

    @staticmethod
    def args(res):
        """Assuming the argument is a binary operation, return all arguments"""
        return res[::2]

    @staticmethod
    def simplify_double_negation(res):
        """Cancel double negations"""
        if CondParser.connective(res) == "!" and CondParser.connective(res[1]) == "!":
            res = res[1][1]
        res = CondParser.map_top(CondParser.simplify_double_negation, res)
        return res

    @staticmethod
    def simplify_not_eq(res):
        """Replace !(x == y) by x != y, and !(x != y) by x == y"""
        if CondParser.connective(res) == "!" and CondParser.connective(res[1]) == "==":
            res = res[1]
            res[1] = "!="
        if CondParser.connective(res) == "!" and CondParser.connective(res[1]) == "!=":
            res = res[1]
            res[1] = "=="
        res = CondParser.map_top(CondParser.simplify_not_eq, res)
        return res

    @staticmethod
    def simplify_neq_chain(res):
        """Check for &&-chains of inequalities followed by an equality
        which implies the inequality. This catches patterns like
        ```
         #if MLKEM_K == 2
         ...
         #elif MLKEM_K == 3
         ...
         #elif MLKEM_K == 4
         ...
         #endif
        ```
        """
        if (
            CondParser.connective(res) == "&&"
            and CondParser.connective(res[-1]) == "=="
        ):
            lhs = res[-1][0]
            rhs = res[-1][2]
            args = []
            for a in CondParser.args(res[:-1]):
                if CondParser.connective(a) == "!=" and a[0] == lhs:
                    args.append(a[2])
                else:
                    args = None
                    break
            if args is None:
                return res
            # Check if all args are numerical and different
            if rhs.isdigit() and all(
                map(lambda a: a.isdigit() and int(a) != int(rhs), args)
            ):
                # Success -- just drop all but the final condition
                return res[-1]
        res = CondParser.map_top(CondParser.simplify_neq_chain, res)
        return res

    @staticmethod
    def print_exp(exp, inner=False):
        conn = CondParser.connective(exp)
        if conn is None:
            return exp
        elif conn == "!":
            res = f"!{CondParser.print_exp(exp[1], inner=True)}"
        else:
            padded_conn = f" {conn} "
            res = padded_conn.join(
                map(lambda e: CondParser.print_exp(e, inner=True), CondParser.args(exp))
            )
        if inner is True and conn in ["&&", "||"]:
            res = f"({res})"
        return res

    def simplify_assoc(exp):
        """Check for unnecesary bracketing and remove it"""
        conn = CondParser.connective(exp)
        if conn in ["&&", "||"]:
            args = CondParser.args(exp)
            new_args = []
            for a in args:
                if CondParser.connective(a) == conn:
                    new_args += CondParser.args(a)
                else:
                    new_args.append(a)
            exp = [x for y in map(lambda x: [x, conn], new_args) for x in y][:-1]
        exp = CondParser.map_top(CondParser.simplify_assoc, exp)
        return exp

    def simplify_all(exp):
        exp = CondParser.simplify_double_negation(exp)
        exp = CondParser.simplify_not_eq(exp)
        exp = CondParser.simplify_neq_chain(exp)
        exp = CondParser.simplify_assoc(exp)
        return exp

    def parse_condition(self, exp, simplify=True):
        try:
            exp = self.parser.parseString(exp, parseAll=True).as_list()[0]
        except pp.ParseException:
            print(f"WARNING: Ignoring condition '{exp}' I cannot parse")
            return exp
        if simplify is True:
            exp = CondParser.simplify_all(exp)
        return exp

    def normalize_condition(self, exp):
        return CondParser.print_exp(self.parse_condition(exp))


def adjust_preprocessor_comments_for_filename(content, source_file, show_status=False):
    """Automatically add comments to large `#if ... #else ... #endif`
    blocks indicating the guarding conditions.

    For example, a block

    ```c
      #if FOO
      ...
      #else
      ...
      #endif
    ```

    will be transformed into


    ```c
      #if FOO
      ...
      #else /* FOO */
      ...
      #endif /* !FOO */
    ```

    except when the distance between the preprocessor directives is
    very short, and the annotations would be more harmful than useful.

    ```
    """
    if show_status:
        status_update("if-else", source_file)

    content = content.split("\n")
    new_content = []

    # Stack of `#if` statements. Every entry is a tuple
    # `(conds, line_no, if_or_else, has_children)`, where
    # - `conds` is the list of conditions being tested.
    #   In a normal `#if ... #else ...` braach, this is a singleton list
    #   containing the condition being tested. In a chain of
    #   `#if .. #elif ..` it contains all conditions encountered to this point.
    # - `line_no` is the line where it started
    # - `if_or_else` indicates whether we are in the `#if`
    #   or the `#else` branch (if present)
    # - `force_print` indicates if a comment should be omitted
    if_stack = []

    def merge_escaped_lines(l, i):
        while l.endswith("\\"):
            l = l.removesuffix("\\").rstrip() + content[i + 1].lstrip()
            i = i + 1
        return (l, i)

    def merge_commented_lines(l, i):
        # Not very robust, but good enough
        if not "/*" in l or "*/" in l:
            return (l, i)
        i += 1
        while "*/" not in content[i]:
            l += content[i]
            i += 1

        l += content[i]
        return (l, i)

    def should_print(cur_line_no, conds, line_no, force_print):
        line_threshold = 5
        if force_print is True:
            return True

        if cur_line_no - line_no >= line_threshold:
            return True
        return False

    parser = CondParser()

    def format_condition(cond):
        cond = re.sub(r"defined\(([^)]+)\)", r"\1", cond)
        return parser.normalize_condition(cond)

    def format_conditions(conds, branch):
        prev_conds = list(map(lambda s: f"!({s})", conds[:-1]))
        final_cond = conds[-1]
        if branch is False:
            final_cond = f"!({final_cond})"
        full_cond = "&&".join(prev_conds + [final_cond])
        return format_condition(full_cond)

    def adhoc_format(line):
        # .c and .h files are formatted as a whole
        if not source_file.endswith(".S"):
            return line
        return format_content(line)

    i = 0
    while i < len(content):
        l = content[i].strip()
        # Replace #ifdef by #if defined(...)
        if l.startswith("#ifdef "):
            l = "#if defined(" + l.removeprefix("#ifdef").strip() + ")"
        if l.startswith("#ifndef "):
            l = "#if !defined(" + l.removeprefix("#ifndef").strip() + ")"
        if l.startswith("#if"):
            l, _ = merge_escaped_lines(l, i)
            cond = l.removeprefix("#if")
            if_stack.append(([cond], i, True, False))
            new_content.append(content[i])
        elif l.startswith("#elif"):
            conds, _, _, force_print = if_stack.pop()
            l, _ = merge_escaped_lines(l, i)
            conds.append(l.removeprefix("#elif"))
            if_stack.append((conds, i, True, force_print))
            new_content.append(content[i])
        elif l.startswith("#else"):
            l, i = merge_escaped_lines(l, i)
            _, i = merge_commented_lines(l, i)
            conds, j, branch, force_print = if_stack.pop()
            assert branch is True
            print_else = should_print(i, cond, j, force_print)
            if_stack.append((conds, i, False, print_else))
            if print_else is True:
                cond = format_conditions(conds, True)
                new_content.append(adhoc_format("#else /* " + cond + " */"))
            else:
                new_content.append("#else")
        elif l.startswith("#endif"):
            l, i = merge_escaped_lines(l, i)
            _, i = merge_commented_lines(l, i)
            conds, j, branch, force_print = if_stack.pop()
            print_endif = should_print(i, conds, j, force_print)
            if print_endif is False:
                new_content.append("#endif")
            else:
                cond = format_conditions(conds, branch)
                new_content.append(adhoc_format("#endif /* " + cond + " */"))
        else:
            # Skip over multiline comments -- we don't want to
            # handle `#if ...` inside documentation as this would
            # lead to nested `/* ... */`.
            i_old = i
            _, i = merge_commented_lines(l, i_old)
            new_content += content[i_old : i + 1]
        i += 1

    return "\n".join(new_content)


def gen_preprocessor_comments_for(source_file, dry_run=False):
    with open(source_file, "r") as f:
        content = f.read()
    new_content = adjust_preprocessor_comments_for_filename(
        content, source_file, show_status=True
    )
    update_file(source_file, new_content, dry_run=dry_run)


def gen_preprocessor_comments(dry_run=False):
    files = (
        get_c_source_files(main_only=True)
        + get_asm_source_files(main_only=True)
        + get_header_files(main_only=True)
    )
    with ThreadPoolExecutor() as executor:
        results = list(
            executor.map(partial(gen_preprocessor_comments_for, dry_run=dry_run), files)
        )


def update_file(
    filename,
    content,
    dry_run=False,
    force_format=False,
    skip_preprocessor_comments=False,
):
    if force_format is True or filename.endswith((".c", ".h", ".i")):
        if skip_preprocessor_comments is False:
            content = adjust_preprocessor_comments_for_filename(content, filename)
        content = format_content(content)

    if os.path.exists(filename) is True:
        with open(filename, "r") as f:
            current_content = f.read()
    else:
        current_content = None

    if current_content == content:
        return

    if dry_run is False:
        file_updated(filename)
        with open(filename, "w+") as f:
            f.write(content)
    else:
        filename_new = f"{filename}.new"
        error(
            f"Autogenerated file {filename} needs updating. Have you called scripts/autogen?"
        )
        info(f"Writing new version to {filename_new}")
        with open(filename_new, "w") as f:
            f.write(content)
        # If the file exists, print diff between old and new version for debugging
        if current_content != None:
            subprocess.run(["diff", filename, filename_new])
        exit(1)


def bitreverse(i, n):
    r = 0
    for _ in range(n):
        r = 2 * r + (i & 1)
        i >>= 1
    return r


def signed_reduce(a):
    """Return signed canonical representative of a mod b"""
    c = a % modulus
    if c >= modulus / 2:
        c -= modulus
    return c


def gen_c_zetas():
    """Generate source and header file for zeta values used in
    the reference NTT and invNTT"""

    # The zeta values are the powers of the chosen root of unity (17),
    # converted to Montgomery form.

    zeta = [0]  # First entry is unused and set to 0
    for i in range(1, 256):
        zeta.append(signed_reduce(pow(root_of_unity, i, modulus) * montgomery_factor))

    # The source code stores the zeta table in bit reversed form
    yield from (zeta[bitreverse(i, 8)] for i in range(256))


def gen_c_zeta_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield "#include <stdint.h>"
        yield ""
        yield "/*"
        yield " * Table of zeta values used in the reference NTT and inverse NTT."
        yield " * See autogen for details."
        yield " */"
        yield "static const int32_t mld_zetas[MLDSA_N] = {"
        yield from map(lambda t: str(t) + ",", gen_c_zetas())
        yield "};"
        yield ""

    update_file("mldsa/zetas.inc", "\n".join(gen()), dry_run=dry_run, force_format=True)


def prepare_root_for_barrett(root):
    """Takes a constant that the code needs to Barrett-multiply with,
    and returns the pair of (a) its signed canonical form, (b) the
    twisted constant used in the high-mul part of the Barrett multiplication."""

    # Signed canonical reduction
    root = signed_reduce(root)

    def round_to_even(t):
        rt = round(t)
        if rt % 2 == 0:
            return rt
        # Make sure to pick a rounding target
        # that's <= 1 away from x in absolute value.
        if rt <= t:
            return rt + 1
        return rt - 1

    root_twisted = round_to_even((root * 2**32) / modulus) // 2
    return root, root_twisted


def gen_aarch64_root_of_unity_for_block(layer, block, inv=False, scale=False):
    # We are computing a negacyclic NTT; the twiddles needed here is
    # the second half of the twiddles for a cyclic NTT of twice the size.
    # For ease of calculating the roots, layers are numbers 0 through 7
    # in this function.
    log = bitreverse(pow(2, layer) + block, 8)
    if inv is True:
        log = -log
    root = pow(root_of_unity, log, modulus)

    if scale is True:
        # Integrate scaling by 2**(-8) and Montgomery factor 2**32 into twiddle
        root = root * pow(2, 32 - 8, modulus)

    root, root_twisted = prepare_root_for_barrett(root)
    return root, root_twisted


def gen_aarch64_fwd_ntt_zetas_layer123456():
    # Layers 1,2,3 are merged
    yield from gen_aarch64_root_of_unity_for_block(0, 0)
    yield from gen_aarch64_root_of_unity_for_block(1, 0)
    yield from gen_aarch64_root_of_unity_for_block(1, 1)
    yield from gen_aarch64_root_of_unity_for_block(2, 0)
    yield from gen_aarch64_root_of_unity_for_block(2, 1)
    yield from gen_aarch64_root_of_unity_for_block(2, 2)
    yield from gen_aarch64_root_of_unity_for_block(2, 3)
    yield from (0, 0)  # Padding

    # Layers 4,5,6 are merged
    for block in range(8):  # There are 8 blocks in Layer 4
        yield from gen_aarch64_root_of_unity_for_block(3, block)
        yield from gen_aarch64_root_of_unity_for_block(4, 2 * block + 0)
        yield from gen_aarch64_root_of_unity_for_block(4, 2 * block + 1)
        yield from gen_aarch64_root_of_unity_for_block(5, 4 * block + 0)
        yield from gen_aarch64_root_of_unity_for_block(5, 4 * block + 1)
        yield from gen_aarch64_root_of_unity_for_block(5, 4 * block + 2)
        yield from gen_aarch64_root_of_unity_for_block(5, 4 * block + 3)
        yield from (0, 0)  # Padding


def gen_aarch64_fwd_ntt_zetas_layer78():
    # Layers 4,5,6,7,8 are merged, but we emit roots for 4,5,6
    # in separate arrays than those for 7,8
    for block in range(8):

        # Ordering of blocks is adjusted to suit the transposed internal
        # presentation of the data

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 0)[i]
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 1)[i]
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 2)[i]
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 3)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 0)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 2)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 4)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 6)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 1)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 3)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 5)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 7)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 0 + 4)[i]
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 1 + 4)[i]
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 2 + 4)[i]
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 3 + 4)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 0 + 8)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 2 + 8)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 4 + 8)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 6 + 8)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 1 + 8)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 3 + 8)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 5 + 8)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 7 + 8)[i]


def gen_aarch64_intt_zetas_layer78():
    for block in range(16):
        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(6, block * 4 + 0, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(6, block * 4 + 1, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(6, block * 4 + 2, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(6, block * 4 + 3, inv=True)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 0, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 2, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 4, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 6, inv=True)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 1, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 3, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 5, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 7, inv=True)[i]


def gen_aarch64_intt_zetas_layer123456():
    for i in range(16):
        yield from gen_aarch64_root_of_unity_for_block(4, i, inv=True)
        yield from gen_aarch64_root_of_unity_for_block(5, i * 2, inv=True)
        yield from gen_aarch64_root_of_unity_for_block(5, i * 2 + 1, inv=True)

    # The last layer has the scaling by 1/256 integrated in the twiddle
    yield from gen_aarch64_root_of_unity_for_block(0, 0, inv=True, scale=True)

    yield from gen_aarch64_root_of_unity_for_block(1, 0, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(1, 1, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 0, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 1, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 2, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 3, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 0, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 1, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 2, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 3, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 4, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 5, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 6, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 7, inv=True)
    yield from (0, 0)  # Padding


def gen_aarch64_zeta_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLD_ARITH_BACKEND_AARCH64)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_aarch64.h"'
        yield ""
        yield "/*"
        yield " * Table of zeta values used in the AArch64 forward NTT"
        yield " * See autogen for details."
        yield " */"
        yield "MLD_ALIGN const int32_t mld_aarch64_ntt_zetas_layer123456[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_fwd_ntt_zetas_layer123456())
        yield "};"
        yield ""
        yield "MLD_ALIGN const int32_t mld_aarch64_ntt_zetas_layer78[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_fwd_ntt_zetas_layer78())
        yield "};"
        yield ""
        yield "MLD_ALIGN const int32_t mld_aarch64_intt_zetas_layer78[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_intt_zetas_layer78())
        yield "};"
        yield ""
        yield "MLD_ALIGN const int32_t mld_aarch64_intt_zetas_layer123456[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_intt_zetas_layer123456())
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLD_EMPTY_CU(aarch64_zetas)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "mldsa/native/aarch64/src/aarch64_zetas.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_aarch64_rej_uniform_eta_table_rows():
    # The index into the lookup table is an 8-bit bitmap, i.e. a number 0..255.
    # Conceptually, the table entry at index i is a vector of 8 16-bit values, of
    # which only the first popcount(i) are set; those are the indices of the set-bits
    # in i. Concretely, we store each 16-bit index as consecutive 8-bit indices.
    def get_set_bits_idxs(i):
        bits = list(map(int, format(i, "08b")))
        bits.reverse()
        return [bit_idx for bit_idx in range(8) if bits[bit_idx] == 1]

    for i in range(256):
        idxs = get_set_bits_idxs(i)
        # Replace each index by two consecutive indices
        idxs = [j for i in idxs for j in [2 * i, 2 * i + 1]]
        # Pad by 255 (invalid index)
        idxs = idxs + [255] * (16 - len(idxs))
        yield idxs


def gen_aarch64_rej_uniform_eta_table(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLD_ARITH_BACKEND_AARCH64) && \\"
        yield "    !defined(MLD_CONFIG_MULTILEVEL_NO_SHARED)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_aarch64.h"'
        yield ""
        yield "/*"
        yield " * Lookup table used by 16-bit rejection sampling (rej_eta)."
        yield " * Adapted from ML-KEM for ML-DSA eta rejection sampling."
        yield " * See autogen for details."
        yield " */"
        yield "MLD_ALIGN const uint8_t mld_rej_uniform_eta_table[] = {"
        for i, idxs in enumerate(gen_aarch64_rej_uniform_eta_table_rows()):
            yield ",".join(map(str, idxs)) + f" /* {i} */,"
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLD_EMPTY_CU(aarch64_rej_uniform_eta_table)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "mldsa/native/aarch64/src/rej_uniform_eta_table.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_aarch64_rej_uniform_table_rows():
    # The index into the lookup table is an 4-bit bitmap, i.e. a number 0..15.
    # Conceptually, the table entry at index i is a vector of 4-bit values, of
    # which only the first popcount(i) are set; those are the indices of the set-bits
    # in i. Concretely, we store each 32-bit index as consecutive 8-bit indices.
    def get_set_bits_idxs(i):
        bits = list(map(int, format(i, "08b")))
        bits.reverse()
        return [bit_idx for bit_idx in range(8) if bits[bit_idx] == 1]

    for i in range(16):
        idxs = get_set_bits_idxs(i)
        # Replace each index by two consecutive indices
        idxs = [j for i in idxs for j in [4 * i + k for k in range(4)]]
        # Pad by -1
        idxs = idxs + [255] * (16 - len(idxs))
        yield idxs


def gen_aarch64_rej_uniform_table(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLD_ARITH_BACKEND_AARCH64) && \\"
        yield "    !defined(MLD_CONFIG_MULTILEVEL_NO_SHARED)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_aarch64.h"'
        yield ""
        yield "/*"
        yield " * Lookup table used by rejection sampling of the public matrix."
        yield " * See autogen for details."
        yield " */"
        yield "MLD_ALIGN const uint8_t mld_rej_uniform_table[] = {"
        for i, idxs in enumerate(gen_aarch64_rej_uniform_table_rows()):
            yield ",".join(map(str, idxs)) + f" /* {i} */,"
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLD_EMPTY_CU(aarch64_rej_uniform_table)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "mldsa/native/aarch64/src/rej_uniform_table.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_avx2_rej_uniform_table_rows():
    # The index into the lookup table is an 8-bit bitmap, i.e. a number 0..255.
    # Conceptually, the table entry at index i is a vector of 8 16-bit values, of
    # which only the first popcount(i) are set; those are the indices of the set-bits
    # in i.
    def get_set_bits_idxs(i):
        bits = list(map(int, format(i, "08b")))
        bits.reverse()
        return [bit_idx for bit_idx in range(8) if bits[bit_idx] == 1]

    for i in range(256):
        idxs = get_set_bits_idxs(i)
        idxs = [i for i in idxs]
        # Pad by 0
        idxs = idxs + [0] * (8 - len(idxs))
        yield "{" + ",".join(map(str, idxs)) + "}"


def gen_avx2_rej_uniform_table(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLD_ARITH_BACKEND_X86_64_DEFAULT) && \\"
        yield "    !defined(MLD_CONFIG_MULTILEVEL_NO_SHARED)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_x86_64.h"'
        yield ""
        yield "/*"
        yield " * Lookup table used by rejection sampling."
        yield " * See autogen for details."
        yield " */"
        yield "MLD_ALIGN const uint8_t mld_rej_uniform_table[256][8] = {"
        yield from map(lambda t: str(t) + ",", gen_avx2_rej_uniform_table_rows())
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLD_EMPTY_CU(avx2_rej_uniform_table)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "mldsa/native/x86_64/src/rej_uniform_table.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def signed_reduce(a):
    """Return signed canonical representative of a mod b"""
    c = a % modulus
    if c >= modulus / 2:
        c -= modulus
    return c


def signed_reduce_u32(a):
    """Return signed canonical representative of a mod b"""
    c = a % 2**32
    if c >= 2**31:
        c -= 2**32
    return c


def prepare_root_for_montmul(root, mult):
    """Takes a constant that the code needs to Montgomery-multiply with,
    and returns the pair of (a) the signed canonical representative of its
    Montgomery form, (b) the twisted constant used in the low-mul part of
    the Montgomery multiplication."""

    # Convert to Montgomery form and pick canonical signed representative
    root = signed_reduce(root * montgomery_factor)
    if mult:
        root = signed_reduce_u32(root * pow(modulus, -1, 2**32))
    return root


def gen_avx2_root_of_unity_for_block(layer, block, mult=False):
    # We are computing a negacyclic NTT; the twiddles needed here is
    # the second half of the twiddles for a cyclic NTT of twice the size.
    log = bitreverse(pow(2, layer) + block, 8)
    root = pow(root_of_unity, log, modulus)
    return prepare_root_for_montmul(root, mult)


def gen_avx2_fwd_ntt_zetas(mult=False):

    def gen_twiddles(layer, block, repeat, mult):
        root = gen_avx2_root_of_unity_for_block(layer, block, mult)
        return [root] * repeat

    def gen_twiddles_many(layer, block_base, block_offsets, repeat, mult):
        roots = list(
            map(
                lambda x: gen_twiddles(layer, block_base + x, repeat, mult),
                block_offsets,
            )
        )
        yield from (r for l in roots for r in l)

    # embed the scaling of 1/256 and correction of the Montgomery factor
    # from the basemul into last twiddle of the inverse NTT
    # - root^-128 * 2^64/256
    # In the forward NTT this twiddle is unused
    f = signed_reduce(-pow(root_of_unity, -128, modulus) * 2**56)
    if mult:
        f = signed_reduce_u32(f * pow(modulus, -1, 2**32))

    yield f

    # Layers 1 twiddle
    # In the inverse NTT this twiddle is unused
    yield from gen_twiddles_many(0, 0, range(1), 1, mult)

    # Layer 2-8 twiddles
    yield from gen_twiddles_many(1, 0, range(2), 1, mult)
    yield from gen_twiddles_many(2, 0, range(4), 1, mult)
    yield from gen_twiddles_many(3, 0, range(8), 4, mult)
    yield from gen_twiddles_many(4, 0, range(16), 2, mult)
    yield from gen_twiddles_many(5, 0, range(32), 1, mult)
    for i in range(32):
        yield from gen_twiddles_many(6, i * 2, range(1), 1, mult)
    for i in range(32):
        yield from gen_twiddles_many(6, i * 2 + 1, range(1), 1, mult)

    for k in range(4):
        for i in range(32):
            yield from gen_twiddles_many(7, i * 4 + k, range(1), 1, mult)


def gen_avx2_zeta_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield "/*"
        yield " * Table of zeta values used in the AVX2 NTTs"
        yield " * See autogen for details."
        yield " */"
        yield "/* twiddles * q^-1 */"
        yield from map(lambda t: str(t) + ",", gen_avx2_fwd_ntt_zetas(mult=True))
        yield "/* twiddles */"
        yield ""
        yield from map(lambda t: str(t) + ",", gen_avx2_fwd_ntt_zetas(mult=False))
        yield ""

    update_file(
        "mldsa/native/x86_64/src/x86_64_zetas.i", "\n".join(gen()), dry_run=dry_run
    )


def get_oqs_shared_sources(backend):
    """Get shared source files for OQS integration"""
    mldsa_dir = "mldsa"

    # add files mldsa/*
    sources = [
        f"mldsa/{f}"
        for f in os.listdir(mldsa_dir)
        if os.path.isfile(f"{mldsa_dir}/{f}") and not f.endswith(".o")
    ]

    if backend != "ref":
        # add files mldsa/native/* (API definitions)
        sources += [
            f"mldsa/native/{f}"
            for f in os.listdir(f"{mldsa_dir}/native")
            if os.path.isfile(f"{mldsa_dir}/native/{f}")
        ]
    # We use a custom config
    sources.remove("mldsa/config.h")
    # Add FIPS202 glue code
    sources += [
        "integration/liboqs/fips202_glue.h",
        "integration/liboqs/fips202x4_glue.h",
    ]
    # Add custom config
    if backend == "ref":
        backend = "c"
    sources.append(f"integration/liboqs/config_{backend.lower()}.h")

    return sources


def get_oqs_native_sources(backend):
    """Get native source files for OQS integration"""
    return [f"mldsa/native/{backend}"]


def gen_oqs_meta_file(filename, dry_run=False):
    """Generate OQS META.yml file with updated source lists"""
    status_update("OQS META", filename)

    with open(filename, "r") as f:
        content = f.read()

    # Parse YAML while preserving structure
    yml_data = yaml.safe_load(content)

    for impl in yml_data["implementations"]:
        name = impl["name"]

        sources = get_oqs_shared_sources(name)

        # NOTE: Sorting at the end causes the libOQS importer to fail.
        # Somehow, the native directory cannot be imported too early.
        sources.sort()

        if name != "ref":
            sources += get_oqs_native_sources(name)
        impl["sources"] = " ".join(sources)

    # Convert back to YAML string with standard copyright header
    yaml_header = "\n".join(gen_yaml_header())

    new_content = yaml.dump(
        yml_data,
        default_flow_style=False,
        sort_keys=False,
        allow_unicode=True,
        encoding=None,
    )

    # Combine copyright header with new YAML content
    new_content = yaml_header + new_content

    update_file(filename, new_content, dry_run=dry_run)


def gen_oqs_meta_files(dry_run=False):
    """Generate all OQS META.yml files"""
    meta_files = [
        "integration/liboqs/ML-DSA-44_META.yml",
        "integration/liboqs/ML-DSA-65_META.yml",
        "integration/liboqs/ML-DSA-87_META.yml",
    ]

    for meta_file in meta_files:
        gen_oqs_meta_file(meta_file, dry_run=dry_run)


def adjust_header_guard_for_filename(content, header_file):

    status_update("header guards", header_file)

    content = content.split("\n")
    exceptions = {}

    # Use full filename as the header guard, with '/' and '.' replaced by '_'
    guard_name = (
        header_file.removeprefix("mldsa/").replace("/", "_").replace(".", "_").upper()
    )
    guard_name = "MLD_" + guard_name

    if header_file in exceptions.keys():
        guard_name = exceptions[header_file]

    def gen_guard():
        yield f"#ifndef {guard_name}"
        yield f"#define {guard_name}"

    def gen_footer():
        yield f"#endif"
        yield ""

    guard = list(gen_guard())
    footer = list(gen_footer())

    # Skip over initial commentary
    insert_at = None
    for i, l in enumerate(content):
        if l.strip() == "" or l.startswith(("/*", " *")):
            continue
        insert_at = i
        break

    i = insert_at
    while content[i].strip() == "":
        i += 1
    # Check if header file has some guard -- if so, drop it
    if content[i].strip().startswith("#if !defined") or content[i].strip().startswith(
        "#ifndef"
    ):
        del content[i]
        if content[i].strip().startswith("#define"):
            del content[i]
        has_guard = True
    else:
        has_guard = False
    # Add standardized guard
    content = content[:i] + guard + content[i:]
    # Check if header has some footer
    if (
        has_guard is True
        and content[-1] == ""
        and content[-2].strip().startswith("#endif")
    ):
        del content[-2:]
    # Add standardized footer
    content = content + footer

    return "\n".join(content)


def gen_header_guard(header_file, dry_run=False):
    with open(header_file, "r") as f:
        content = f.read()
    new_content = adjust_header_guard_for_filename(content, header_file)
    update_file(header_file, new_content, dry_run=dry_run)


def gen_header_guards(dry_run=False):
    with ThreadPoolExecutor() as executor:
        _ = list(
            executor.map(
                partial(gen_header_guard, dry_run=dry_run),
                get_header_files(main_only=True),
            )
        )


def get_markdown_files():
    return get_files("**/*.md")


def gen_yaml_header():
    yield "# Copyright (c) The mldsa-native project authors"
    yield "# SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT"
    yield ""


class BibliographyEntry:
    def __init__(self, raw_dict):
        self._raw = raw_dict
        self._usages = []

    def register_usages(self, lst):
        self._usages += lst

    @property
    def usages(self):
        return self._usages

    @property
    def name(self):
        return self._raw["name"]

    @property
    def short(self):
        if "short" in self._raw.keys():
            return self._raw["short"]
        return self.name

    @property
    def id(self):
        return self._raw["id"]

    @property
    def url(self):
        return self._raw["url"]

    @staticmethod
    def full_name(name):
        if "," not in name:
            return name
        surname, forename = name.split(",")
        return forename.strip() + " " + surname.strip()

    @property
    def authors(self):
        authors = self._raw["author"]
        if not isinstance(authors, list):
            authors = [authors]
        authors = list(map(BibliographyEntry.full_name, authors))
        return authors

    @property
    def authors_text(self):
        authors = self._raw["author"]
        if not isinstance(authors, list):
            authors = [authors]

        def surname(name):
            return name.split(",")[0].strip()

        if len(authors) > 1:
            authors = ", ".join(map(surname, authors))
        else:
            authors = BibliographyEntry.full_name(authors[0])
        return authors


def gen_markdown_citations_for(filename, bibliography, dry_run=False):

    # Skip BIBLIOGRAPHY.md
    if filename == "BIBLIOGRAPHY.md":
        return

    with open(filename, "r") as f:
        content = f.read()
    content = content.split("\n")

    # Lookup all citations in style `[^ID]`
    citations = {}
    for i, l in enumerate(content):
        for m in re.finditer(r"\[\^(?P<id>\w+)\]", l):
            cite_id = m.group("id")
            uses = citations.get(cite_id, [])
            uses.append((filename, i))
            citations[cite_id] = uses

    # Find and remove any existing citation footnotes
    footnote_footer_start = "<!--- bibliography --->"
    try:
        i = content.index(footnote_footer_start)
        content = content[:i]
    except ValueError:
        pass

    # Add footnotes for all citations found
    if len(citations) > 0:
        content.append(footnote_footer_start)
    cite_ids = list(citations.keys())
    cite_ids.sort()
    for cite_id in cite_ids:
        uses = citations[cite_id]
        entry = bibliography.get(cite_id, None)
        if entry is None:
            raise Exception(
                f"Could not find bibliography entry {cite_id} referenced in {filename}. Known entries: {list(bibliography.keys())}"
            )
        content.append(
            f"[^{cite_id}]: {entry.authors_text}: {entry.name}, [{entry.url}]({entry.url})"
        )

        # Remember this usage of the bibliography entry
        entry.register_usages(uses)

    if len(citations) > 0:
        content.append("")

    update_file(filename, "\n".join(content), dry_run=dry_run)


def gen_c_citations_for(filename, bibliography, dry_run=False):

    with open(filename, "r") as f:
        content = f.read()

    references_start = [
        "/* References",
        " * ==========",
    ]
    references_end = [" */"]

    # Find and remove any existing reference section
    ref_pattern = r"/\* (# )?References.*?\*/\n+"
    content = re.sub(ref_pattern, "", content, flags=re.DOTALL)

    content = content.split("\n")

    # Lookup all citations in style `@[ID]`
    citations = {}
    for i, l in enumerate(content):
        for m in re.finditer(r"@\[(?P<id>\w+)", l):
            cite_id = m.group("id")
            uses = citations.get(cite_id, [])
            # Remember usage. +1 because line counting starts at 1
            uses.append((filename, i + 1))
            citations[cite_id] = uses

    # Add references section
    references = []
    references += references_start

    cite_ids = list(citations.keys())
    cite_ids.sort()
    for cite_id in cite_ids:
        uses = citations[cite_id]
        entry = bibliography.get(cite_id, None)
        if entry is None:
            raise Exception(
                f"Could not find bibliography entry {cite_id} referenced in {filename}"
            )
        references.append(f" *")
        references.append(f" * - [{cite_id}]")
        references.append(f" *   {entry.name}")
        references.append(f" *   {entry.authors_text}")
        references.append(f" *   {entry.url}")

    references += references_end

    # Pre-update formatting messes up overly long comment lines which are
    # likely here. Thus, we explicitly format and fix bad indentation manually.
    references = "\n".join(references)
    references = format_content(references)
    references = re.sub(r" \* (\w.*)", r" *   \1", references)
    references = references.split("\n")
    references = [""] + references

    if len(cite_ids) > 0:
        # Add references to file after initial header section
        # Skip over copyright
        insert_at = None
        for i, l in enumerate(content):
            if l.startswith(("/*", " *")):
                continue
            insert_at = i
            break
        content = content[:insert_at] + references + content[insert_at:]

    # Remember uses -- needs to happen after insertion of references
    # since we need to adjust the line count
    for cite_id in cite_ids:
        uses = citations[cite_id]
        entry = bibliography.get(cite_id, None)

        # Adjust line count after insertion of references
        def bump_line_count(x):
            return (x[0], x[1] + len(references))

        uses = list(map(bump_line_count, uses))

        # Remember this usage of the bibliography entry
        entry.register_usages(uses)

    update_file(
        filename, "\n".join(content), dry_run=dry_run, skip_preprocessor_comments=True
    )


def gen_citations_for(filename, bibliography, dry_run=False):
    status_update("citations", filename)
    if filename.endswith(".md"):
        gen_markdown_citations_for(filename, bibliography, dry_run=dry_run)
    elif filename.endswith((".c", ".h", ".S")):
        gen_c_citations_for(filename, bibliography, dry_run=dry_run)
    else:
        raise Exception(f"Unexpected file extension in {filename}")


def gen_bib_file(bibliography, dry_run=False):

    content = [
        "[//]: # (SPDX-License-Identifier: CC-BY-4.0)",
        "[//]: # (This file is auto-generated from BIBLIOGRAPHY.yml)",
        "[//]: # (Do not modify it directly)",
        "",
        "# Bibliography",
        "",
        "This file lists the citations made throughout the mldsa-native ",
        "source code and documentation.",
        "",
    ]

    cite_ids = list(bibliography.keys())
    cite_ids.sort()

    for cite_id in cite_ids:
        entry = bibliography[cite_id]
        content.append(f"### `{cite_id}`")
        content.append("")
        content.append(f"* {entry.name}")
        content.append(f"* Author(s):")
        for author in entry.authors:
            content.append(f"  - {author}")
        content.append(f"* URL: {entry.url}")
        content.append(f"* Referenced from:")
        # Usages are pairs of (filename, line_count)
        # Ignore line_count for now, as it would require `autogen` after
        # a change to source files.
        files = list(set(map(lambda x: x[0], entry.usages)))
        files.sort()
        for filename in files:
            content.append(f"  - [{filename}]({filename})")
        content.append("")

    update_file("BIBLIOGRAPHY.md", "\n".join(content), dry_run=dry_run)


def gen_citations(dry_run=False):
    # Load bibliography
    with open("BIBLIOGRAPHY.yml", "r") as f:
        bibliography_raw = yaml.safe_load(f.read())

    bibliography = {}
    for r in bibliography_raw:
        cite_id = r["id"]
        bibliography[cite_id] = BibliographyEntry(r)

    with ThreadPoolExecutor() as executor:
        _ = list(
            executor.map(
                partial(
                    gen_citations_for,
                    bibliography=bibliography,
                    dry_run=dry_run,
                ),
                get_markdown_files()
                + get_asm_source_files()
                + get_c_source_files()
                + get_header_files(),
            )
        )

    # Check that every bibliography entry has been used as least once
    for e in bibliography.values():
        if len(e.usages) == 0:
            raise Exception(
                f"Bibliography entry {e.id} is unused! "
                "Add a citation or remove from BIBLIOGRAPHY.yml."
            )

    gen_bib_file(bibliography, dry_run=False)


def _main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("--dry-run", default=False, action="store_true")

    args = parser.parse_args()

    os.chdir(os.path.join(os.path.dirname(__file__), ".."))

    gen_citations(args.dry_run)
    high_level_status("Generated citations")
    gen_oqs_meta_files(args.dry_run)
    high_level_status("Generated OQS META.yml files")
    gen_c_zeta_file(args.dry_run)
    gen_aarch64_zeta_file(args.dry_run)
    gen_aarch64_rej_uniform_table(args.dry_run)
    gen_aarch64_rej_uniform_eta_table(args.dry_run)
    gen_avx2_zeta_file(args.dry_run)
    gen_avx2_rej_uniform_table(args.dry_run)
    high_level_status("Generated zeta and lookup tables")
    gen_header_guards(args.dry_run)
    high_level_status("Generated header guards")
    gen_preprocessor_comments(args.dry_run)
    high_level_status("Generated preprocessor comments")


if __name__ == "__main__":
    _main()
